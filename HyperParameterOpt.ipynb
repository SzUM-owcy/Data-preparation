{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Flatten,Dense,MaxPooling2D,Conv2D,Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import train, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_normalize = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "test_normalize = ImageDataGenerator(rescale=1./255)\n",
    "val_normalize = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "main_dir = 'splits/split2_new/'\n",
    "train_set = train_normalize.flow_from_directory(directory=main_dir+\"train\",target_size=(225,225),class_mode='categorical')\n",
    "val_set = val_normalize.flow_from_directory(directory=main_dir+\"val\",target_size=(225,225),class_mode=\"categorical\")\n",
    "test_set = test_normalize.flow_from_directory(directory=main_dir+\"test\",target_size=(225,225),class_mode=\"categorical\")\n",
    "\n",
    "fig_size = 225\n",
    "checkpoint_dir = 'checkpoints/Optimization'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer = 'adam',learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), input_shape=(fig_size, fig_size, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # if optimizer == 'adam':\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    # else :\n",
    "    # opt = SGD(learning_rate=learning_rate, momentum=0.9) #lol\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "batch_sizes = reversed([8 , 32])\n",
    "epochs = 25\n",
    "#learning_rates = [0.001,0.01, 0.1]\n",
    "# learning_rate = 0.001\n",
    "#optimizers = ['adam', 'sgd']\n",
    "\n",
    "# (X_train,Y_train) = train_set\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# print(model.get_params().keys())\n",
    "#,learning_rate=learning_rates,\n",
    "#param_grid = dict( optimizer = optimizers,batch_size=batch_sizes)\n",
    "# param_grid = dict( batch_size=batch_sizes)\n",
    "\n",
    "# grid = GridSearchCV(estimator=model,param_grid=param_grid, n_jobs=3,cv=3)\n",
    "# grid_result = grid.fit(X_train,Y_train)\n",
    "# model = create_model()\n",
    "histories = []\n",
    "for batch_size in batch_sizes:\n",
    "    model = create_model()\n",
    "    checkpoint_path = os.path.join(checkpoint_dir,\"MODEL4-BatchSize\"+str(batch_size)+\"-{epoch:1d}.weights.h5\")\n",
    "    os.makedirs(checkpoint_dir,exist_ok=True)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,  # Save only the weights\n",
    "        save_freq='epoch', # Save for every epoch,\n",
    "        save_best_only = True\n",
    "    )\n",
    "    histories=(model.fit(train_set, epochs=epochs, batch_size=batch_size, validation_data=val_set, callbacks=[checkpoint_callback]))\n",
    "    \n",
    "\n",
    "\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "\n",
    "\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Custom tick formatter function\n",
    "def format_tick(x, pos):\n",
    "    return int(x + 1)\n",
    "# Plot training accuracy and loss\n",
    "\n",
    "for history in histories:\n",
    "    plt.figure(figsize=(12, 6), constrained_layout=True)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', color='red')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.grid(True)  # Add grid\n",
    "    plt.xticks(range(0, epochs+1))  # Set x-axis ticks to every integer value\n",
    "    plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(format_tick))  # Apply custom tick formatter\n",
    "    plt.tight_layout()  # Adjust layout\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot validation accuracy and loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', color='red')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)  # Add grid\n",
    "    plt.xticks(range(0, epochs+1))  # Set x-axis ticks to every integer value\n",
    "    plt.gca().xaxis.set_major_formatter(plt.FuncFormatter(format_tick))  # Apply custom tick formatterplt.tight_layout()  # Adjust layout\n",
    "    plt.show()\n",
    "    plt.tight_layout(pad=5.0)\n",
    "    plt.subplots_adjust(left=0.1, bottom=0.1,                    top=0.9, wspace=0.4,hspace=0.4)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_normalize = ImageDataGenerator(rescale=1./255)\n",
    "test_normalize = ImageDataGenerator(rescale=1./255)\n",
    "val_normalize = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "main_dir = 'splits/split2_new/'\n",
    "train_set = train_normalize.flow_from_directory(directory=main_dir+\"train\",target_size=(225,225),class_mode='categorical', shuffle=False)\n",
    "val_set = val_normalize.flow_from_directory(directory=main_dir+\"val\",target_size=(225,225),class_mode=\"categorical\", shuffle=False)\n",
    "test_set = test_normalize.flow_from_directory(directory=main_dir+\"test\",target_size=(225,225),class_mode=\"categorical\", shuffle=False)\n",
    "\n",
    "fig_size = 225\n",
    "checkpoint_dir = 'checkpoints/Optimization'\n",
    "\n",
    "filepath = os.path.join(checkpoint_dir,f\"MODEL4-BatchSize{32}-{23}.weights.h5\")\n",
    "model = create_model()\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "# Calculate metrics based on confusion matrix\n",
    "\n",
    "data_sets = [train_set,val_set,test_set] \n",
    "data_sets_names = [\"Train\",\"Validation\",\"Test\"]\n",
    "for dataset in data_sets:\n",
    "\n",
    "    predicted_labels = np.argmax(model.predict(dataset), axis=1)\n",
    "    accuracy = (accuracy_score(dataset.classes, predicted_labels))\n",
    "    recall = (recall_score(dataset.classes, predicted_labels, average='macro'))\n",
    "    precision =(precision_score(dataset.classes, predicted_labels, average='macro'))\n",
    "    f1 = (f1_score(dataset.classes, predicted_labels, average='macro'))\n",
    "\n",
    "    print(data_sets_names[data_sets.index(dataset)])\n",
    "    print(f\"Accuracy :{accuracy}\")\n",
    "    print(f\"Recall :{recall}\")\n",
    "    print(f\"Precision :{precision}\")\n",
    "    print(f\"F1 :{f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
